# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains demographics about people who were called by a bank's marketing team, when the call took place, and for how long, among other things.
We seek to predict the column ambiguously labeled "y", which I'm going to guess is the outcome of whether a product was successfully sold after each call.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was a Voting Ensemble generated by AutoML.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

Pipeline consisted of:
1. downloading, cleaning, and splitting the dataset into train and test partitions
2. configuring HyperDrive
3. running Scikit-learn LogisticRegression repeatedly with HyperDrive

For data partitioning, I used default options for sklearn.train_test_split, which results in a 75/25 train/test ratio.
For HyperDriveConfig, I expanded the search space to 0.5x and 2x the default settings for "C" and "max_iter".

**What are the benefits of the parameter sampler you chose?**

Random and Bayesian support continuous parameter spaces, which I wanted for floating point input "C", as I have no intuition on what discrete choices to give to Grid.
Bayesian apparently doesn't support early termination, so I wouldn't be able to get full credit on the assignment with it (unless no termination counts as a policy?).
Therefore RandomParameterSampling.

**What are the benefits of the early stopping policy you chose?**

I went with BanditPolicy using a fairly generous tolerance of 0.2, since I wanted to give more slack to each run in case of "late-bloomers".
Hard no to TruncationSelectionPolicy, as I'd like to run all 20 iterations if possible.
MedianStoppingPolicy seems fine as a conservative terminator as well.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

VotingEnsemble used weighted voting between instances of LightGBM, LogisticRegression, and ExtremeRandomTrees.
Weights and iterations are recorded at the bottom of the notebook for posterity.
It used "duration" and "emp.var.rate" as the most important features, with "nr.employed" coming in at a distant third.
The only real choice I had to make for my AutoMLConfig was k-fold cross validation, which I set to 4 to approximate the default 75/25 split from sklearn.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

HyperDrive tuning found a model with 0.9146637533381888 accuracy, while AutoML VotingEnsemble achieved 0.9174808897187983 accuracy, so it was a fairly close competition.  In fact, most of the AutoML iterations had worse accuracy, sometimes significantly worse.
Architecture-wise, (and I hope I'm interpreting the model correctly) the VotingEnsemble was approximately 1/3 LogisticRegression and 60% LightGBM.  MaxAbsScaler-LightGBM was the reigning AutoML champion for a long time at 0.9167 accuracy, which already beat sklearn's pure LogisticRegression.  So it makes some sense that a solution dominated by those 2 models would achieve accuracy at least as good as logistic regression alone.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

HyperDrive was bounded by number of iterations, while AutoML was bounded by time duration.
For HyperDrive I'd want to run more models, and on a wider parameter space, to get a better understanding of the sweet spots.  Then maybe switch to Bayesian sampling to converge on parameter ranges that look promising.
For AutoML, I'd want to try to squeeze more models in the allotted time, because the ensembles didn't kick in until the very end, and were clear winners.  What other promising permutations did I miss out on?  I don't remember if early termination policies are specifiable, but I did see a time limit for individual runs.  Also, k-fold cross validation could be reduced to speed things up, while hopefully not degrading accuracy too much.

P.S. I also struggled greatly trying to save the models, especially with HyperDrive, so more prep on that front would've helped with the time pressure.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

![delete_compute](https://user-images.githubusercontent.com/80217508/110264577-990cd680-7f87-11eb-9e56-c6664094e5e7.png)
